{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1294c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen \n",
    "from urllib.error import URLError\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1891afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(link):\n",
    "    '''Возвращает экземпляр BeautifulSoup для соответсвующей ссылки'''\n",
    "    page = urlopen(link)\n",
    "    html_bytes = page.read()\n",
    "    html = html_bytes.decode(\"utf-8\")\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a2f6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(vacancies, vac_dict):\n",
    "    '''\n",
    "    Обходит все вакансии на странице и добавляет извлеченные \n",
    "    данные в соотвествующий список\n",
    "    '''\n",
    "    # Создаем регулярное выражение для парсинга данных о ЗП\n",
    "    regex = re.compile('vacancy-salary-compensation-type-.*')\n",
    "    # Итерируем по списку вакансий\n",
    "    for vacancy in vacancies:\n",
    "        link = vacancy.find('a')['href']\n",
    "        # Подготавливаем soup\n",
    "        vac_soup = create_soup(link)\n",
    "        # Извлекаем местоположение и отрасль\n",
    "        try:\n",
    "            js = json.loads(\"\".join(vac_soup.find(\"script\", {\"type\":\"application/ld+json\"}).contents))\n",
    "            vac_dict['location'].append(js['jobLocation']['address']['addressLocality'])\n",
    "            vac_dict['industry'].append(' '.join(js['industry']))\n",
    "        except:\n",
    "            continue\n",
    "        # Извлекаем ЗП\n",
    "        try:\n",
    "            vac_dict['salary'].append(vac_soup.find('span', {'data-qa':regex}).get_text())   \n",
    "        except:\n",
    "            vac_dict['salary'].append(None)\n",
    "        # Извлекаем нываки\n",
    "        try:\n",
    "            skills = vac_soup.findAll('div',{'data-qa':'bloko-tag bloko-tag_inline skills-element'})\n",
    "            skil = [s.get_text() for s in skills]\n",
    "            vac_dict['skills'].append(' '.join(skil)) \n",
    "        except:\n",
    "            vac_dict['skills'].append(None)\n",
    "        # Извлекаем опыт\n",
    "        try:\n",
    "            vac_dict['experience'].append(vac_soup.find('span',{'data-qa':'vacancy-experience'}).get_text())\n",
    "        except: \n",
    "            vac_dict['experience'].append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60b066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n",
      "Произошла ошибка! Пропуск страницы...\n"
     ]
    }
   ],
   "source": [
    "url = 'https://hh.ru/search/vacancy?area=113&search_field=description&only_with_salary=true&text=&hhtmFrom=vacancy_search_list'\n",
    "vac_dict = {'location':[], 'industry':[], 'experience': [], 'skills':[], 'salary':[]}\n",
    "pages = 40\n",
    "# Итерируем по страницам\n",
    "for page in range(pages):\n",
    "    page_link = url + '&page=' + str(page)\n",
    "    try:\n",
    "        # Подготавливаем soup\n",
    "        page_soup = create_soup(page_link)\n",
    "        # Извлекаем все вакансии на странице\n",
    "        vacancies = page_soup.find_all('div', class_='vacancy-serp-item-body')\n",
    "        # Передаем полученный список в функцию для сбора данных о каждой вакансии\n",
    "        process_page(vacancies, vac_dict)\n",
    "    except:\n",
    "        print('Произошла ошибка! Пропуск страницы...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3062c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразовываем словарь в DataFrame\n",
    "vacancies = pd.DataFrame.from_dict(vac_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "eed1a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Записываем DataFrame в csv файл\n",
    "vacancies.to_csv('vacancies.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
